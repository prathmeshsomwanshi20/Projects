{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361ce61e-cfc1-4d49-a604-909d605d0642",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_text = \"Artificial Intelligence (AI) is transforming numerous industries by enhancing efficiencies and creating new opportunities. From healthcare to transportation, AI applications are helping businesses optimize their processes and deliver better services. For instance, AI-driven algorithms can analyze vast amounts of data to uncover insights that humans might miss. In healthcare, AI is being used for accurate diagnostics and personalized medicine. Moreover, autonomous vehicles, powered by AI, promise to revolutionize the way we commute and transport goods. However, as AI technology advances, it also poses ethical challenges, including concerns about privacy and job displacement. Society must navigate these challenges thoughtfully to leverage AI's full potential while minimizing risks. Overall, the impact of AI on the modern world is profound and multifaceted, making it a pivotal part of future innovations.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae565a-382e-4023-9469-0e4cc727e73c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7563ac03-809b-4557-97e6-4d0016f0b1ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;66;03m#The re library in Python stands for \"regular expression\" and is a powerful tool used for string searching and manipulation. It provides functions to work with patterns to match, search, or modify strings. Includes matching pattern, splitting strings, subsituting texts, etc.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\__init__.py:146\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjsontags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# PACKAGES\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[1;32m--> 146\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\__init__.py:155\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunkers\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# For license information, see LICENSE.TXT\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m     pattern is valid.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkParserI\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnamed_entity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Maxent_NE_Chunker\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregexp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\api.py:13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m##  Chunk Parser Interface\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m##//////////////////////////////////////////////////////\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChunkScore\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParserI\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\chunk\\util.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy \u001b[38;5;28;01mas\u001b[39;00m _accuracy\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmapping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_tag\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tree\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:72\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TaggerI\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m str2tuple, tuple2str, untag\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     73\u001b[0m     SequentialBackoffTagger,\n\u001b[0;32m     74\u001b[0m     ContextTagger,\n\u001b[0;32m     75\u001b[0m     DefaultTagger,\n\u001b[0;32m     76\u001b[0m     NgramTagger,\n\u001b[0;32m     77\u001b[0m     UnigramTagger,\n\u001b[0;32m     78\u001b[0m     BigramTagger,\n\u001b[0;32m     79\u001b[0m     TrigramTagger,\n\u001b[0;32m     80\u001b[0m     AffixTagger,\n\u001b[0;32m     81\u001b[0m     RegexpTagger,\n\u001b[0;32m     82\u001b[0m     ClassifierBasedTagger,\n\u001b[0;32m     83\u001b[0m     ClassifierBasedPOSTagger,\n\u001b[0;32m     84\u001b[0m )\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTagger\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbrill_trainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BrillTaggerTrainer\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tag\\sequential.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional, Tuple\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m jsontags\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NaiveBayesClassifier\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConditionalFreqDist\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtag\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\classify\\__init__.py:97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpositivenaivebayes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PositiveNaiveBayesClassifier\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrte_classify\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscikitlearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SklearnClassifier\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msenna\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Senna\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassify\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtextcat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextCat\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\classify\\scikitlearn.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprobability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictionaryProbDist\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictVectorizer\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_set_output\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\__init__.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib\n",
      "File \u001b[1;32msklearn\\utils\\murmurhash.pyx:1\u001b[0m, in \u001b[0;36minit sklearn.utils.murmurhash\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import re #The re library in Python stands for \"regular expression\" and is a powerful tool used for string searching and manipulation. It provides functions to work with patterns to match, search, or modify strings. Includes matching pattern, splitting strings, subsituting texts, etc.\n",
    "import nltk #The Natural Language Toolkit, commonly known as NLTK, is a comprehensive library in Python used for natural language processing (NLP) and computational linguistics. POS tagging, named entity recognition, parsing, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8112f775-0bc1-4492-91cb-df515a6744ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"artificial intelligence (ai) is transforming numerous industries by enhancing efficiencies and creating new opportunities. from healthcare to transportation, ai applications are helping businesses optimize their processes and deliver better services. for instance, ai-driven algorithms can analyze vast amounts of data to uncover insights that humans might miss. in healthcare, ai is being used for accurate diagnostics and personalized medicine. moreover, autonomous vehicles, powered by ai, promise to revolutionize the way we commute and transport goods. however, as ai technology advances, it also poses ethical challenges, including concerns about privacy and job displacement. society must navigate these challenges thoughtfully to leverage ai's full potential while minimizing risks. overall, the impact of ai on the modern world is profound and multifaceted, making it a pivotal part of future innovations.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_text = actual_text.lower()\n",
    "actual_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1cd06f-7491-4873-ba42-d6d8494e436b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'artificial intelligence ai is transforming numerous industries by enhancing efficiencies and creating new opportunities from healthcare to transportation ai applications are helping businesses optimize their processes and deliver better services for instance ai driven algorithms can analyze vast amounts of data to uncover insights that humans might miss in healthcare ai is being used for accurate diagnostics and personalized medicine moreover autonomous vehicles powered by ai promise to revolutionize the way we commute and transport goods however as ai technology advances it also poses ethical challenges including concerns about privacy and job displacement society must navigate these challenges thoughtfully to leverage ai s full potential while minimizing risks overall the impact of ai on the modern world is profound and multifaceted making it a pivotal part of future innovations '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text = re.sub('[^a-zA-Z]', ' ', actual_text) # this line says if any character is not an alphabet then replace it with the space.\n",
    "clean_text = re.sub('\\s+', ' ', clean_text) # Now due to your above work there may be generated many white spaces so it aggregates it and make it a single white space.\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e5ad2c0-4ee1-409b-af4b-a98f6c92dcf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentence_list \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39msent_tokenize(actual_text) \u001b[38;5;66;03m# Now basically using the sent_tokenize functionality of nltk we are creating sentences as distinct objects from the text and nltk already has pre defined models that can logicall set the boundaries of the sentences.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sentence_list\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_list = nltk.sent_tokenize(actual_text) # Now basically using the sent_tokenize functionality of nltk we are creating sentences as distinct objects from the text and nltk already has pre defined models that can logicall set the boundaries of the sentences.\n",
    "sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede7fd5b-baa8-4bba-9fb3-2bc552034e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # Here we are downloading all the stopwords at once. Stop words are the ones which are less important like and, the or this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "083660e2-9064-4619-a14c-1957548660c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english') # import all the stop words from english\n",
    "\n",
    "word_frequencies = {} # just created a dictionary that will store words and their frequency\n",
    "for word in nltk.word_tokenize(clean_text): # split the clean text into individual words or tokens\n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies:\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dce9fe23-5242-41ff-a9d0-846c9dc5bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_frequency = max(word_frequencies.values()) # In this overall code we find the word with maximum frequency and note it's frequency value and then \n",
    "# we iterate in the dictionary and divide each frequency value with max_frequency to normalize all values and bring them in between 0 and 1\n",
    "\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = word_frequencies[word] / maximum_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9631341-eca0-47d2-906b-1e3f59e83465",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_scores = {}\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word in word_frequencies and len(sentence.split(' ')) < 30: # here we check if the sentence contain a significant work from earlier declared dictionary and then \n",
    "            # if the sentence has less than 30 occurences of that word then accept it.\n",
    "            if sentence not in sentence_scores:\n",
    "                sentence_scores[sentence] = word_frequencies[word]\n",
    "            else:\n",
    "                sentence_scores[sentence] += word_frequencies[word]\n",
    "# here in above if else we are just including each sentence in the scores named dictionary and and keeping the count of frequencies of all words that are present in the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dce48c-5276-45e5-ad67-ff572048f270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificial': 0.125,\n",
       " 'intelligence': 0.125,\n",
       " 'ai': 1.0,\n",
       " 'transforming': 0.125,\n",
       " 'numerous': 0.125,\n",
       " 'industries': 0.125,\n",
       " 'enhancing': 0.125,\n",
       " 'efficiencies': 0.125,\n",
       " 'creating': 0.125,\n",
       " 'new': 0.125,\n",
       " 'opportunities': 0.125,\n",
       " 'healthcare': 0.25,\n",
       " 'transportation': 0.125,\n",
       " 'applications': 0.125,\n",
       " 'helping': 0.125,\n",
       " 'businesses': 0.125,\n",
       " 'optimize': 0.125,\n",
       " 'processes': 0.125,\n",
       " 'deliver': 0.125,\n",
       " 'better': 0.125,\n",
       " 'services': 0.125,\n",
       " 'instance': 0.125,\n",
       " 'driven': 0.125,\n",
       " 'algorithms': 0.125,\n",
       " 'analyze': 0.125,\n",
       " 'vast': 0.125,\n",
       " 'amounts': 0.125,\n",
       " 'data': 0.125,\n",
       " 'uncover': 0.125,\n",
       " 'insights': 0.125,\n",
       " 'humans': 0.125,\n",
       " 'might': 0.125,\n",
       " 'miss': 0.125,\n",
       " 'used': 0.125,\n",
       " 'accurate': 0.125,\n",
       " 'diagnostics': 0.125,\n",
       " 'personalized': 0.125,\n",
       " 'medicine': 0.125,\n",
       " 'moreover': 0.125,\n",
       " 'autonomous': 0.125,\n",
       " 'vehicles': 0.125,\n",
       " 'powered': 0.125,\n",
       " 'promise': 0.125,\n",
       " 'revolutionize': 0.125,\n",
       " 'way': 0.125,\n",
       " 'commute': 0.125,\n",
       " 'transport': 0.125,\n",
       " 'goods': 0.125,\n",
       " 'however': 0.125,\n",
       " 'technology': 0.125,\n",
       " 'advances': 0.125,\n",
       " 'also': 0.125,\n",
       " 'poses': 0.125,\n",
       " 'ethical': 0.125,\n",
       " 'challenges': 0.25,\n",
       " 'including': 0.125,\n",
       " 'concerns': 0.125,\n",
       " 'privacy': 0.125,\n",
       " 'job': 0.125,\n",
       " 'displacement': 0.125,\n",
       " 'society': 0.125,\n",
       " 'must': 0.125,\n",
       " 'navigate': 0.125,\n",
       " 'thoughtfully': 0.125,\n",
       " 'leverage': 0.125,\n",
       " 'full': 0.125,\n",
       " 'potential': 0.125,\n",
       " 'minimizing': 0.125,\n",
       " 'risks': 0.125,\n",
       " 'overall': 0.125,\n",
       " 'impact': 0.125,\n",
       " 'modern': 0.125,\n",
       " 'world': 0.125,\n",
       " 'profound': 0.125,\n",
       " 'multifaceted': 0.125,\n",
       " 'making': 0.125,\n",
       " 'pivotal': 0.125,\n",
       " 'part': 0.125,\n",
       " 'future': 0.125,\n",
       " 'innovations': 0.125}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3be9f2-4dca-490f-866b-20d14d84952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'artificial intelligence (ai) is transforming numerous industries by enhancing efficiencies and creating new opportunities.': 2.25,\n",
       " 'from healthcare to transportation, ai applications are helping businesses optimize their processes and deliver better services.': 2.375,\n",
       " 'for instance, ai-driven algorithms can analyze vast amounts of data to uncover insights that humans might miss.': 1.375,\n",
       " 'in healthcare, ai is being used for accurate diagnostics and personalized medicine.': 1.875,\n",
       " 'moreover, autonomous vehicles, powered by ai, promise to revolutionize the way we commute and transport goods.': 2.25,\n",
       " 'however, as ai technology advances, it also poses ethical challenges, including concerns about privacy and job displacement.': 2.625,\n",
       " \"society must navigate these challenges thoughtfully to leverage ai's full potential while minimizing risks.\": 2.375,\n",
       " 'overall, the impact of ai on the modern world is profound and multifaceted, making it a pivotal part of future innovations.': 2.375}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e31ede0-ed49-497a-93d8-c0e5d424c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "however, as ai technology advances, it also poses ethical challenges, including concerns about privacy and job displacement. from healthcare to transportation, ai applications are helping businesses optimize their processes and deliver better services. society must navigate these challenges thoughtfully to leverage ai's full potential while minimizing risks. overall, the impact of ai on the modern world is profound and multifaceted, making it a pivotal part of future innovations. artificial intelligence (ai) is transforming numerous industries by enhancing efficiencies and creating new opportunities.\n"
     ]
    }
   ],
   "source": [
    "import heapq # using heapq functionality you can say the element with highest score will be in the front and then 2nd highest second and that's all.\n",
    "summary = heapq.nlargest(5, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "print(\" \".join(summary))\n",
    "# just extract the top five sentences based on their scores and then join them in a single string. nlargest method gives 5 largest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a263be61-4e38-4aa9-884d-3cff0d4e6f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\anaconda3\\lib\\site-packages (from textstat) (68.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from cmudict->textstat) (7.0.1)\n",
      "Collecting importlib-resources>=5 (from cmudict->textstat)\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.17.0)\n",
      "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.3 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 61.4/105.3 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 105.3/105.3 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 153.6/939.4 kB 2.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 204.8/939.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 266.2/939.4 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 327.7/939.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 460.8/939.4 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 522.2/939.4 kB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 645.1/939.4 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 727.0/939.4 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 819.2/939.4 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  931.8/939.4 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 939.4/939.4 kB 1.9 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.2/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.1 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.6/2.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.3/2.1 MB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.4/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.5/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.8/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.0/2.1 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.6 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pyphen, importlib-resources, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.32 importlib-resources-6.5.2 pyphen-0.17.2 textstat-0.7.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8628c01-c1c6-4e73-b2f8-03f68c4a2f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flesch-Kincaid Grade Level: 15.4\n",
      "This text is better for college-level readers.\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "# Calculate readability\n",
    "readability_score = textstat.flesch_kincaid_grade(actual_text)\n",
    "\n",
    "print(f\"Flesch-Kincaid Grade Level: {readability_score}\")\n",
    "if readability_score < 6:\n",
    "    print(\"This text is suitable for a lower reading level.\")\n",
    "elif 6 <= readability_score < 12:\n",
    "    print(\"This text is suitable for middle school to high school students.\")\n",
    "else:\n",
    "    print(\"This text is better for college-level readers.\")\n",
    "\n",
    "#Flesch-Kincaid Grade Level: Basically it is a US based technique that is used to find the complexity in text they have their own formula of finding complexity using words, syllabis and all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f29e029b-2592-4a5c-82ca-77ef82f04160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spaCy is a popular open-source library for advanced natural language processing (NLP) in Python.\n",
    "#1. For high speed data access and manipulation\n",
    "#2. Pre-trained models\n",
    "#3. Easy Integration.\n",
    "#Load the model: Load the English language model en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794e417-c59c-4188-b4e2-43736de4feb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that involves locating and classifying named entities in text into predefined categories, \n",
    "#such as persons, organizations, locations, dates, and more. Works by tokenization, POS tagging and then entity recognition.\n",
    "#Entity: Apple Inc., Label: ORG\n",
    "#Entity: New York City, Label: GPE\n",
    "#Entity: Tim Cook, Label: PERSON\n",
    "#Entity: Apple, Label: ORG\n",
    "#Entity: September 25, 2023, Label: DATE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
